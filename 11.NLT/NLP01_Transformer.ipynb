{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMszWSJGQueT2ToO3sgEy7x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhportfolio1/MsAiSchool11-Language/blob/main/NLP01_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihqwCFuSPvj_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 자연어처리 기본\n",
        "- RNN, DRNN 등의 방법이 있음"
      ],
      "metadata": {
        "id": "QV2M3TajQl_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 1) 번역처리 RNN\n",
        ">- seq2seq(attention 메커니즘 존재 및 사용) : 인코더와 디코더로 이루어져 있음\n",
        ">- 문장이 하나의 벡터로만 표기함\n",
        ">- 순차적으로 계산하니, 문장이 길어졌을때 앞부분 소실 가능"
      ],
      "metadata": {
        "id": "TLYK37HpQoxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 2) 번역 Transformer\n",
        ">- 2017년 구글 논문이후 계속적으로 적용됨"
      ],
      "metadata": {
        "id": "IBrNoqJ-aN7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Transformer (est 2017)"
      ],
      "metadata": {
        "id": "2_T5tqOAQ_DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 1) seq2seq\n",
        ">- 특징 : RNN 사용하지 않고, attention 기법을 사용함 (구글 : \"attention is all you need\" 논문 적용)"
      ],
      "metadata": {
        "id": "kOstpWCQRJa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (1) attention\n",
        ">>- 인코더-디코더 구조를 설계하였지만 RNN보다 우수함\n",
        ">>- 인코더가 해석하기 적합한 wieght을 찾는 과정 : 쿼리, 키, 값 모두 인코더에서 나옴(디코더의 어텐션과 차이남)\n",
        ">>- 인코더-디코더 단위가 N개 존재할 수 있음\n",
        ">>- 토큰화 : [나는, 밥을, 먹었다] -> {'나는':25, '밥을':111, ...} 식으로 벡터로 변환되어 저장됨\n",
        ">>- 보카에 3만여개 정도 -> 토큰을 하나씩 벡터로 저장함 "
      ],
      "metadata": {
        "id": "e5wjh1DqRb9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (2) Self-Attention\n",
        ">>- 인코더 스스로 집중해야 하는 부분을 찾아내서 집중함\n",
        ">>- 집중하는 부분 : 가중치(softmax) 높게 부여함 / 행렬곱\n",
        ">>- [A,B] : 2+2\n",
        ">>- [A, B, C, D] : 4+4+4+4 \n",
        ">>- 연산량에 따른 복잡도 : 제곱으로 증가함 (GPU 부담 증가)\n",
        ">>- ChatGPT 등에 사용됨\n"
      ],
      "metadata": {
        "id": "Q9U9Rcl_Sfle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (3) Multi-Head Attention\n",
        ">>- attention을 여러번 하는 것\n",
        ">>- 한 주제에 대해 여러 관점으로 해석하여 정확성 증대함\n",
        ">>- 병렬처리, linear 차원축소 : attention과 시간은 똑같음"
      ],
      "metadata": {
        "id": "aFOMhmy9UEou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. BERT vs GPT\n",
        "- 인코더와 디코더를 따로 쓰는 모델\n",
        "- 언어를 사전 학습 시키는 것이 주 목적임"
      ],
      "metadata": {
        "id": "jlxmqj9eY2XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 1) BERT\n",
        ">- Transformer의 인코더만 사용\n",
        ">- 프리트레이닝(사전훈련), 파인튜닝(특정 데이터 훈련과정)\n",
        ">- 대량의 말뭉치를 프리트레이닝하고, 레이블링 데이터로 파인튜닝을 했더니 상당히 높은 정확도 가짐\n",
        ">- 검증 : my [ ] is cute and he likes playing 에서 빈칸 맞추기"
      ],
      "metadata": {
        "id": "_p5tM27qY-pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 2) GPT\n",
        ">- Generative Pre-Training\n",
        ">- Transformer의 디코더만 사용\n",
        ">- Masked Self-attention에서 input의 전의 것만 참조하도록 만듦\n",
        ">- 검증 : my [ ] 일때, my 다음에 나올 수 있는 토큰을 예측하게함"
      ],
      "metadata": {
        "id": "YkSBLduVablI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (1) GPT1\n",
        ">>- 다양한 스페셜 토큰을 활용해 파인튜닝 성능 극대화함"
      ],
      "metadata": {
        "id": "Tm8s0Wz7bQ4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (2)GPT2\n",
        ">>- 수백만 웹페이지를 외부 감독없이 QA, 기계번역, 문맥읽기 등에 활용함"
      ],
      "metadata": {
        "id": "7uBqArnTbZAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (3)GPT3\n",
        ">>- 1750억 파라미터로 훈련함\n",
        ">>- Few-Shot Learning 강조 : 프롬프트에 예시를 몇개를 줌 (숫자에 따라 Zeroshot~Tenshot 등으로 이야기함)"
      ],
      "metadata": {
        "id": "ZmIAzzDvbhIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>### (4)트레이닝 데이터\n",
        ">>- CommonCrawl 데이터 -> high quality reference로 한번 필터링해서 중복되는 부분 없앰"
      ],
      "metadata": {
        "id": "Gp0TwmnfcDnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Pytorch"
      ],
      "metadata": {
        "id": "mJlLaA6rd4ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">## 1) Tensor Basic\n",
        ">- 다차원 배열로 벡터와 행렬 일반화 (3차원 이상 확장가능)\n",
        ">- Numpy와 유사하고, torch로 만들때 변환과정이 필요함\n",
        ">- 계산 : Backpropagation / Loss를 줄이기 위해 타겟값과 실제 모델 결과 사이의 오차값을 역으로 전파해가며 노드 변수 갱신함"
      ],
      "metadata": {
        "id": "uvVW1Tp0d68M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTNpvma2QoT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}